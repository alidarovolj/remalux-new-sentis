План исследования разработки Unity-приложения для сегментации и моментальной перекраски стенI. ВведениеA. Цель и задачи исследованияДанный документ представляет собой план исследования, направленного на анализ разработки Unity-приложения, обладающего функциями сегментации стен в реальном времени и их моментальной перекраски в указанный пользователем цвет. Исследование фокусируется исключительно на этих двух ключевых функциях, аналогичных возможностям приложения Dulux Visualizer. Основная цель — определить наиболее эффективные подходы, технологии и методологии для реализации такого функционала на мобильных устройствах с использованием движка Unity. Задачи исследования включают анализ существующих моделей машинного обучения для семантической сегментации, изучение методов их интеграции в Unity, разработку техник визуализации перекраски с сохранением текстур и исследование способов улучшения визуальной достоверности и пользовательского опыта в контексте дополненной реальности (AR). Поскольку исходный репозиторий remalux-final оказался недоступен 1, анализ будет основываться на общедоступной информации и релевантных технологиях.B. Методология исследованияМетодология исследования базируется на анализе существующей научно-технической литературы, документации по программным продуктам и репозиториям с открытым исходным кодом. Будут рассмотрены научные статьи, посвященные семантической сегментации, алгоритмам компьютерного зрения и методам рендеринга в реальном времени. Особое внимание будет уделено документации Unity Technologies, в частности, пакетам Sentis (ранее Barracuda) для инференса нейронных сетей, Universal Render Pipeline (URP) для создания пользовательских шейдеров и эффектов постобработки, а также AR Foundation для разработки приложений дополненной реальности. Анализ репозиториев, таких как Unity Barracuda Samples 2 и примеры использования MobileSAM 3, поможет понять практические аспекты интеграции.C. Структура отчетаОтчет структурирован следующим образом:
Введение: Определение целей, задач и методологии исследования.
Сегментация стен в реальном времени на мобильных устройствах: Анализ моделей машинного обучения, их выбор и реализация в Unity с использованием Sentis.
Моментальная перекраска стен в Unity: Разработка шейдерных техник для перекраски с сохранением текстур и интеграция с маской сегментации.
Улучшение визуальной достоверности и пользовательского опыта в AR: Использование AR Foundation, стабилизация маски сегментации и реалистичное освещение.
Поэтапный план исследований и разработок: Предложение фаз разработки с ключевыми задачами и результатами.
Заключительные рекомендации: Обобщение рекомендуемых технологий, приоритетных направлений и потенциальных рисков.
II. Сегментация стен в реальном времени на мобильных устройствахРазработка функции сегментации стен в реальном времени для мобильных приложений требует тщательного выбора модели машинного обучения и эффективной ее интеграции в среду Unity. Ключевыми факторами являются точность сегментации, скорость инференса на мобильных процессорах и размер модели.A. Выбор модели для сегментации стенВыбор подходящей модели машинного обучения для сегментации стен является критически важным этапом. Основные критерии включают точность идентификации стен, скорость работы на мобильных устройствах с ограниченными ресурсами, размер самой модели (влияющий на размер приложения и время загрузки) и возможность дообучения (fine-tuning) на специфическом датасете стен, если это потребуется.

Критерии выбора модели:

Точность (Accuracy): Способность модели корректно выделять пиксели, принадлежащие стенам, минимизируя ложные срабатывания и пропуски. Метрики, такие как Mean Intersection over Union (mIoU), являются стандартными для оценки.
Скорость инференса (Inference Speed): Время, необходимое модели для обработки одного кадра. Для мобильных AR-приложений критично достижение скорости, обеспечивающей плавность работы (например, 15-30 FPS).
Размер модели (Model Size): Компактность модели важна для уменьшения общего размера приложения и сокращения времени инициализации.
Пригодность для мобильных устройств (Mobile Suitability): Архитектура модели должна быть оптимизирована для работы на мобильных CPU и GPU.
Специфичность для стен (Wall Specificity): Наличие предварительно обученных весов для сегментации стен или возможность эффективного дообучения на таком датасете.



Обзор кандидатов:

MobileUNet: Является легковесной архитектурой сверточной нейронной сети (CNN), специально разработанной для задач семантической сегментации на мобильных и встраиваемых устройствах.5 Она представляет собой вариант популярной архитектуры U-Net, оптимизированный для сред с ограниченными ресурсами. MobileUNet использует глубинные разделяемые свертки (depthwise separable convolutions) и узкие блоки (bottleneck blocks) для снижения вычислительной сложности и улучшения скорости обработки.5 Это делает ее подходящей для приложений реального времени. Ключевым преимуществом является баланс между высокой точностью сегментации и эффективностью. Хотя в статье 5 нет явного упоминания об оптимизации MobileUNet конкретно для больших плоских поверхностей, таких как стены, ее общая способность к сегментации окружения для AR-приложений позволяет предположить ее пригодность.
MobileSAM (Mobile Segment Anything Model): Компактная и эффективная модель сегментации изображений, созданная для мобильных и периферийных устройств.3 Она призвана перенести возможности модели Segment Anything Model (SAM) от Meta в среды с ограниченными вычислительными ресурсами. MobileSAM заменяет большой кодировщик ViT-H (632M параметров) из оригинальной SAM на гораздо меньший кодировщик Tiny-ViT (5M параметров), что приводит к уменьшению размера модели примерно в 5 раз и увеличению скорости примерно в 7 раз.3 Размер модели составляет 40.7 МБ, количество параметров — 10.1 млн. Однако ее скорость на CPU (около 25381 мс на изображение) значительно ниже, чем у YOLOv8n-seg.3 MobileSAM наследует способность SAM к сегментации "чего угодно" на основе подсказок (точек или рамок), а не предварительного обучения на конкретные категории. Для использования в контексте сегментации стен может потребоваться комбинация с другими моделями или дообучение, если это поддерживается.6
YOLOv8n-seg (и аналогичные YOLO-модели): Модели семейства YOLO (You Only Look Once) известны своей высокой скоростью и эффективностью для задач детекции объектов и семантической/инстансной сегментации.8 Например, YOLOv8n-seg значительно меньше (6.7 МБ) и быстрее (24.5 мс на изображение на CPU) по сравнению с MobileSAM.3 Модели YOLO обычно обучаются для конкретных задач детекции и сегментации, что делает их высокооптимизированными для этих категорий. Для сегментации стен потребуется либо найти предварительно обученную на стенах модель YOLO, либо дообучить существующую на соответствующем датасете. Ultralytics предоставляет инструменты для обучения и экспорта моделей YOLO, включая формат ONNX.9



Сравнительный анализ и предварительный выбор:


ХарактеристикаMobileUNetMobileSAMYOLOv8n-segАрхитектураU-Net с оптимизациями (глубинные свертки) 5SAM с Tiny-ViT кодировщиком 3YOLO-архитектура для сегментации 9Размер моделиЛегковесная (зависит от конфигурации) 540.7 МБ 36.7 МБ 3Скорость (CPU)Оптимизирована для реального времени 5~25381 мс/изображение 3~24.5 мс/изображение 3ТочностьВысокая для мобильных устройств 5Зависит от подсказок, общая сегментация 3Высокая для обученных классов 9Специфичность (стены)Требует дообучения/поиска весовТребует подсказок или дообучения для категорий 3Требует дообучения/поиска весовONNX ЭкспортВозможен (например, из PyTorch) 10Возможен 4Поддерживается Ultralytics 9ПреимуществаЭффективность, масштабируемость 5Сегментация "чего угодно", совместимость с SAM 3Очень высокая скорость, малый размер 3НедостаткиМожет требовать значительного дообученияНизкая скорость CPU, не категорийная по умолчанию 3Требует качественного датасета для дообучения
Для первоначальной разработки и прототипирования **YOLOv8n-seg** представляется наиболее перспективным кандидатом из-за наилучшего сочетания скорости и размера, что критично для мобильных AR-приложений. Однако, если основной задачей является сегментация произвольных поверхностей без привязки к категориям, MobileSAM может быть интересен, но его производительность на CPU вызывает опасения. MobileUNet является надежным выбором, если удастся найти хорошо обученные веса для стен или если есть ресурсы для качественного дообучения.


Дообучение (Fine-tuning) для класса "стена":

Независимо от выбранной базовой модели, скорее всего, потребуется дообучение для точной сегментации именно стен в разнообразных условиях (различное освещение, наличие мебели, текстуры стен).
Процесс дообучения включает сбор или поиск датасета изображений с аннотированными стенами.
Для моделей типа SAM/MobileSAM, дообучение может фокусироваться на декодере масок или адаптации к специфическим подсказкам для стен.6
Для YOLO и MobileUNet это стандартный процесс дообучения на новом классе или датасете.9
Важно учитывать, что качество и объем датасета для дообучения напрямую влияют на конечную точность сегментации.

Необходимость дообучения модели специально для сегментации стен является важным аспектом. Хотя модели общего назначения, такие как MobileSAM, могут сегментировать "что угодно" при наличии правильных подсказок 3, их производительность для конкретного класса "стена" без дообучения может быть недостаточной или требовать сложных эвристик для генерации подсказок. Модели, такие как YOLOv8n-seg или MobileUNet, изначально ориентированы на классификацию, и их дообучение на датасете стен позволит им научиться специфическим признакам этого класса.5 Процесс дообучения, хотя и трудоемкий из-за необходимости сбора и разметки данных, часто приводит к значительному улучшению точности и надежности для целевой задачи.
Более того, выбор модели должен учитывать доступность инструментов для дообучения и экспорта в формат ONNX. Большинство современных фреймворков глубокого обучения (PyTorch, TensorFlow) поддерживают экспорт в ONNX.10 Например, для моделей из 🤗 Transformers (включая варианты, на которых может базироваться MobileUNet или компоненты SAM) существует 🤗 Optimum для экспорта.10 Для YOLO-моделей фреймворк Ultralytics предоставляет встроенные средства экспорта.9

B. Реализация в Unity с использованием SentisUnity Sentis (ранее известный как Barracuda) является библиотекой для выполнения нейронных сетей непосредственно в Unity, поддерживая кроссплатформенность и работу как на CPU, так и на GPU.14 Это делает его идеальным инструментом для интеграции выбранной модели сегментации.

Обзор Unity Sentis:

Sentis позволяет импортировать обученные модели нейронных сетей в Unity и запускать их в реальном времени.16
Поддерживается импорт моделей в формате ONNX (Open Neural Network Exchange) с версиями opset от 7 до 15.16 Это обеспечивает совместимость с моделями, экспортированными из популярных фреймворков, таких как PyTorch и TensorFlow.14
Sentis доступен для всех пользователей Unity через Package Manager и находится в состоянии "релиз" 16, в отличие от предыдущих версий Barracuda или ранних бета-версий Sentis, которые были экспериментальными или требовали приглашения.15



Процесс экспорта модели в ONNX:

Модель, обученная или дообученная в выбранном фреймворке (например, PyTorch для MobileUNet/MobileSAM или Ultralytics для YOLO), должна быть экспортирована в формат ONNX.
Для моделей на основе 🤗 Transformers можно использовать 🤗 Optimum.10
Для MobileSAM существуют примеры экспорта, разделяющие модель на кодировщик и декодер.4
Для nnU-Net (родственной U-Net архитектуры) также существуют наработки по экспорту в ONNX, хотя и с оговорками об ответственности пользователя за создание пайплайна.11
Ultralytics YOLO предоставляет команду model.export(format="onnx") для простого экспорта.9
Важно убедиться, что все операции модели поддерживаются целевой версией opset ONNX и движком Sentis.16



Импорт и запуск ONNX модели в Unity с помощью Sentis:

Файл .onnx импортируется в проект Unity. Sentis автоматически обрабатывает его и создает ModelAsset.
Загрузка модели и создание исполнителя (Worker):
C#// Загрузка ModelAsset из проекта
public ModelAsset modelAsset;
private Model runtimeModel;
private IWorker worker;

void Start()
{
    // Загрузка модели во время выполнения
    runtimeModel = ModelLoader.Load(modelAsset);
    // Создание исполнителя для CPU или GPU
    // BackendType.GPUCompute, BackendType.GPUPixel, BackendType.CPU
    worker = WorkerFactory.CreateWorker(runtimeModel, BackendType.GPUCompute);
}

Пример основан на общей практике использования Sentis/Barracuda.14
Исполнитель (IWorker) используется для запуска модели с входными данными (тензорами) и получения выходных данных.



Подготовка входного тензора для модели сегментации:

Модели сегментации изображений обычно ожидают входной тензор в определенном формате, например, NCHW (Batch Size, Channels, Height, Width) или NHWC (Batch Size, Height, Width, Channels).20
Данные изображения (например, с камеры Unity Texture2D или RenderTexture) должны быть преобразованы в тензор TensorFloat.
Это включает изменение размера изображения до ожидаемого моделью, нормализацию значений пикселей (например, в диапазон  или [-1, 1] и стандартизацию по среднему и стандартному отклонению, если это требуется моделью).
Sentis предоставляет TextureConverter.ToTensor() для преобразования текстур Unity в тензоры.19 Можно указать TextureTransform для управления размерами, обрезкой, и порядком каналов.
C#// Пример подготовки входного тензора из Texture2D
public Texture2D inputImage;
private TensorFloat inputTensor;

//... в Start или где необходимо подготовить тензор...
// Размеры должны соответствовать ожиданиям модели
int modelWidth = 256;
int modelHeight = 256;
// Создание тензора нужной формы (например, NCHW: 1, 3 канала RGB, высота, ширина)
inputTensor = new TensorFloat(new TensorShape(1, 3, modelHeight, modelWidth));
// Преобразование Texture2D в TensorFloat с необходимой трансформацией
TextureTransform txTransform = new TextureTransform().SetDimensions(modelWidth, modelHeight, 3);
TextureConverter.ToTensor(inputImage, inputTensor, txTransform);


Документация Sentis указывает, что если формат тензора не соответствует ожиданиям модели, результаты могут быть непредсказуемыми, и для изменения формата можно использовать функциональный API Sentis или IOps.Transpose.20



Обработка выходного тензора (маски сегментации):

Модель сегментации обычно выводит тензор, содержащий информацию о классах для каждого пикселя. Это могут быть:

Сырые значения (логиты) для каждого класса.
Вероятности принадлежности к каждому классу (после Softmax).
Индекс класса с максимальной вероятностью для каждого пикселя.


Формат выходного тензора может быть, например, или.
Для получения бинарной маски для класса "стена" необходимо:

Получить выходной тензор от worker.PeekOutput().19
Если на выходе вероятности или логиты, найти индекс класса "стена".
Выбрать значения для этого класса и применить пороговое значение (thresholding) для получения бинарной маски (0 или 1).
Эта маска затем может быть преобразована в Texture2D или RenderTexture для использования в шейдере перекраски.


Sentis предоставляет TextureConverter.ToTexture() или TextureConverter.RenderToTexture() для преобразования выходного тензора в текстуру.19
C#// Пример обработки выходного тензора (упрощенно)
// TensorFloat outputTensor = worker.PeekOutput() as TensorFloat;
// outputTensor.MakeReadable(); // Если нужен доступ к данным на CPU

// Предположим, outputTensor имеет форму [1, NumClasses, H, W]
// и содержит вероятности. Нам нужен класс "стена" (например, class_idx = 1).

// Этот этап может быть выполнен на GPU с помощью Compute Shader для эффективности
// или на CPU для прототипирования.
// Здесь будет логика извлечения маски для класса "стена" и бинаризации.
// Результатом должна быть текстура (например, RenderTexture) с маской.
// public RenderTexture segmentationMaskTexture;
// TextureConverter.RenderToTexture(processed_mask_tensor, segmentationMaskTexture);


Документация Meta для Sentis в Horizon OS упоминает, что для моделей YOLO выходные тензоры могут содержать координаты объектов и ID классов, и для постобработки потребуются пользовательские скрипты.18 Это подчеркивает необходимость понимания точной структуры выходных данных конкретной модели.

Крайне важно, чтобы процесс преобразования данных изображения в тензор и последующая обработка выходного тензора модели были максимально эффективными. Прямой доступ к элементам тензора на CPU (myTensor) может вызывать блокирующие операции чтения/записи с GPU, что негативно скажется на производительности.20 Для высокопроизводительных приложений рекомендуется использовать асинхронные операции чтения (ReadOutputAsync 23), Compute Shaders или Burst Compiler для обработки данных непосредственно на GPU или с использованием оптимизированного нативного кода. Функциональный API Sentis также может быть использован для модификации модели с целью изменения формата выходных данных или их постобработки на GPU до передачи на CPU.20
Кроме того, структура выходного тензора модели сегментации может варьироваться. Некоторые модели могут выводить тензор вероятностей для каждого класса на пиксель (например, [1, C, H, W]), в то время как другие могут напрямую выводить тензор с индексами классов ([1, 1, H, W]). Понимание этой структуры, указанной в документации к модели или определенной при ее экспорте, является ключом к правильной постобработке. Например, если модель выводит вероятности, потребуется операция ArgMax по оси классов для получения наиболее вероятного класса для каждого пикселя, а затем фильтрация по индексу класса "стена". Если же модель выводит сырые логиты, сначала потребуется применить функцию Softmax. Эти операции могут быть добавлены как слои в саму ONNX-модель перед экспортом или реализованы с помощью функционального API Sentis или Compute Shaders в Unity.

III. Моментальная перекраска стен в UnityПосле получения маски сегментации стен следующим шагом является их моментальная перекраска в выбранный пользователем цвет. Это достигается с помощью шейдерных техник, которые должны не только изменять цвет, но и по возможности сохранять оригинальные текстурные детали и учитывать освещение.A. Разработка шейдера для перекраски стенОсновой для перекраски стен служит полноэкранный эффект постобработки, реализуемый в Unity с использованием Universal Render Pipeline (URP). Этот эффект будет применять новый цвет к областям, указанным маской сегментации, поверх оригинального изображения с камеры.

Метод реализации: Полноэкранный эффект постобработки в URP:

В URP пользовательские эффекты постобработки обычно реализуются с помощью ScriptableRendererFeature и ScriptableRenderPass.24 ScriptableRendererFeature добавляет один или несколько ScriptableRenderPass в конвейер рендеринга.
ScriptableRenderPass будет выполнять операцию полноэкранного копирования (blit) с использованием специального материала и шейдера. Шейдер будет принимать на вход оригинальный цветовой буфер камеры (например, _CameraOpaqueTexture, предоставляемый URP, если включена соответствующая опция в URP Asset 26) и текстуру маски сегментации.
Пример использования Blitter.BlitCameraTexture в ScriptableRenderPass демонстрирует, как применять полноэкранный шейдер к изображению с камеры.25



Сохранение оригинальных текстур и деталей стены:

Простое наложение плоского цвета сделает перекрашенную стену неестественной. Важно сохранить видимость оригинальных текстур (узоры обоев, фактура краски, мелкие неровности) под новым цветом.
Техники в шейдере:

Режимы смешивания (Blend Modes): Использование режимов смешивания, таких как Multiply, Overlay, Soft Light или других, для комбинации нового цвета с существующим цветом пикселя из _CameraOpaqueTexture. Выбор режима зависит от желаемого визуального эффекта.
Колоризация с сохранением яркости (Luminance-Preserving Colorization): Этот подход позволяет изменять оттенок и насыщенность, сохраняя исходную яркость пикселя. Это помогает сохранить детали текстуры, которые проявляются через вариации света и тени.

Конвертация цвета оригинального пикселя и нового цвета краски в цветовое пространство, разделяющее яркость и цветность (например, HSL, HSV, YCbCr или YIQ).
Модификация компонент оттенка (Hue) и насыщенности (Saturation) на основе нового цвета краски, при этом сохраняя оригинальную компоненту яркости (Luminance/Value/Y).
В 27 обсуждается использование цветового пространства YIQ для сохранения воспринимаемой яркости, что является более продвинутым и потенциально лучшим подходом. Координата Y в YIQ кодирует воспринимаемую яркость, а координаты I и Q совместно кодируют оттенок и насыщенность. Манипулируя I и Q, можно изменить оттенок, не затрагивая Y.
Реализации преобразований RGB в HSL/HSV/YIQ и обратно существуют для HLSL/CG.27


Доступ к картам нормалей/деталей (продвинутый уровень): Если бы можно было оценить или получить карты нормалей или деталей оригинальной стены (например, грубую геометрию от ARMeshManager 29), их можно было бы использовать для модуляции перекрашенного цвета для большей реалистичности. Это выходит за рамки базовой перекраски и связано с более глубокой интеграцией AR (см. Раздел IV.C).


Уроки 30 и 31 демонстрируют примеры шейдеров для рисования в Unity, где применяется цвет, и один из них упоминает сохранение деталей с помощью карт нормалей.



Трансформация и управление цветом:

Пользователь будет выбирать целевой цвет (например, с помощью палитры цветов). Этот цвет должен передаваться в шейдер как uniform-переменная.
Необходимо обеспечить согласованность цветовых пространств (например, sRGB vs. Linear) между пользовательским интерфейсом Unity, шейдером и данными с камеры. URP в значительной степени управляет этим, но это важный аспект, требующий внимания.
Подход с использованием цветового пространства YIQ 27 особенно интересен для сохранения воспринимаемой яркости при изменении оттенка, что критично для реализма. Шейдер будет конвертировать RGB в YIQ, манипулировать I и Q (оттенок/насыщенность), сохранять Y (яркость) и конвертировать обратно в RGB.



Таблица 2: Техники шейдера для перекраски стен


Название техникиВходные текстуры шейдераМетод применения цветаПодход к сохранению текстурыВзаимодействие с освещениемПлюсыМинусыСложность реализацииПолноэкранный постобработочный эффект с URP Renderer Feature_CameraOpaqueTexture, Маска сегментации (_WallMask)lerp между оригинальным и целевым цветомОтсутствует (плоский цвет)Базовое (цвет не зависит от освещения стены)Простота реализацииНеестественный вид, потеря деталейНизкаяСмешивание цветов (Blend Modes)_CameraOpaqueTexture, _WallMaskРежимы смешивания (Multiply, Overlay, etc.)Частичное, зависит от режима смешивания и исходной текстурыБазовоеУлучшенный вид по сравнению с lerp, некоторые детали сохраняютсяРезультат сильно зависит от исходного цвета стены и выбранного режимаСредняяМанипуляция HSL/HSV_CameraOpaqueTexture, _WallMask, Целевой цвет (_PaintColor)Замена H, S; сохранение L/VХорошее сохранение вариаций яркости, теней, световых бликовБазовое (яркость оригинальная, цвет новый)Реалистичное сохранение текстуры, контроль над насыщенностью и оттенкомМожет привести к неестественным цветам при экстремальных изменениях H/SСредняя-ВысокаяМанипуляция YIQ/YCbCr_CameraOpaqueTexture, _WallMask, _PaintColorМанипуляция I, Q (или Cb, Cr); сохранение YОтличное сохранение воспринимаемой яркости и деталейБазовое (яркость оригинальная, цвет новый)Наиболее реалистичное сохранение текстуры и яркости 27Требует точных преобразований цветовых пространствВысокаяПродвинутое освещение с картами нормалей AR_CameraOpaqueTexture, _WallMask, _PaintColor, Карта нормалей ARКак YIQ + учет нормалей для освещенияОтличное + реакция на освещениеПродвинутое (реагирует на виртуальное/AR освещение)Максимальный реализм, краска взаимодействует с геометрией и светомЗависит от качества данных AR (ARMeshManager), сложность 29Очень высокая
Выбор конкретной техники колоризации в шейдере, например, простое наложение цвета в сравнении с манипуляциями в пространствах HSL/YIQ для сохранения яркости, напрямую влияет на воспринимаемый реализм и удовлетворенность пользователя. Этот фактор может оказаться более значимым, чем незначительные неточности в сегментации. Плоское наложение цвета может сделать перекрашенную стену искусственной, похожей на наклейку. Сохранение же оригинальной текстуры стены и вариаций ее яркости (теней, едва заметных узоров) позволяет новому цвету выглядеть более интегрированным в сцену.[27, 30, 31] Техники, подобные преобразованиям в цветовое пространство YIQ [27], специально разработаны для изменения оттенка и насыщенности при сохранении воспринимаемой яркости, что является ключевым для достижения реализма. Следовательно, инвестиции в разработку более сложного шейдера для нанесения цвета, даже если он будет несколько более ресурсоемким, могут принести большую отдачу с точки зрения визуального качества и принятия пользователем, чем, например, стремление к дополнительным 1-2% в метрике mIoU сегментации, если сама перекраска выглядит плоской.

Если ставится задача продвинутого сохранения текстуры (например, с использованием оригинальных карт нормалей или деталей рельефа), это создает обратную связь к системе сегментации/AR. От нее потребуется предоставлять больше, чем просто 2D-маску (например, данные о глубине, нормалях поверхности от `ARMeshManager`, как обсуждается в [29]). Это повышает сложность задачи за пределы простого экранного эффекта. Базовое сохранение текстуры опирается на существующие цвета в `_CameraOpaqueTexture`. Чтобы новая краска реагировала на мелкие детали поверхности *оригинальной* стены (например, текстурированные обои, небольшие неровности), шейдеру потребуется информация о геометрии/материале этой оригинальной поверхности, например, ее карта нормалей. Эта информация отсутствует в `_CameraOpaqueTexture` или простой 2D-маске сегментации. Для этого потребуются данные от AR-систем, таких как реконструкция сцены ARKit (`ARMeshManager` [29]), которые могут предоставлять геометрию сетки с нормалями для отсканированных поверхностей. Таким образом, желаемый уровень реализма перекраски (простое тонирование цвета против реакции краски на оригинальные детали поверхности) диктует требуемые входные данные от конвейера AR/сегментации. Решение в пользу более высокого реализма здесь значительно увеличивает объем работ по интеграции AR в проект, потенциально требуя устройств с LiDAR для достижения наилучших результатов.
B. Интеграция маски сегментации с шейдером перекраскиКлючевым элементом системы является передача маски сегментации, полученной от нейронной сети, в шейдер перекраски, чтобы цвет изменялся только на целевых участках изображения.

Передача текстуры маски в шейдер:

Бинарная маска сегментации (сгенерированная на этапе II.B.5, вероятно, как RenderTexture или Texture2D) должна быть доступна шейдеру перекраски.
В URP, при использовании ScriptableRendererFeature, C#-скрипт может устанавливать эту текстуру для материала, используемого полноэкранным проходом.25 Пример: material.SetTexture("_SegmentationMask", maskTexture);.
Шейдер затем объявляет sampler2D _SegmentationMask; (или TEXTURE2D(_SegmentationMask); SAMPLER(sampler_SegmentationMask); в HLSL) для доступа к ней. Имя текстуры в шейдере (_SegmentationMask) должно совпадать с именем, используемым при вызове SetTexture.



Использование маски для условной перекраски:

Внутри фрагментного шейдера производится выборка из текстуры маски по текущим экранным координатам пикселя.
Значение маски (например, 1 для стены, 0 для не-стены, или значение в диапазоне  если маска нестрого бинарная) используется для условного применения логики перекраски.

Пример: float maskVal = SAMPLE_TEXTURE2D(_SegmentationMask, sampler_SegmentationMask, i.uv).r;
finalColor = lerp(originalColor, paintedColor, maskVal);


Это гарантирует, что перекрашиваются только пиксели, идентифицированные моделью сегментации как "стена".



Вопросы производительности:

Разрешение текстуры маски должно быть соответствующим. Оно может быть ниже разрешения экрана, если выход модели сегментации меньше, а затем масштабироваться.
Необходимо минимизировать накладные расходы на передачу и выборку этой дополнительной текстуры. Полноэкранные проходы в URP обычно эффективны, но каждая дополнительная выборка текстуры увеличивает нагрузку на GPU.

"Жесткость" или "мягкость" краев маски сегментации и то, как они обрабатываются в условной логике (например, lerp) в шейдере, существенно повлияют на визуальный переход между окрашенными и неокрашенными областями. Необработанные маски сегментации от моделей иногда могут иметь алиасинг (зубчатые края). Если шейдер использует жесткий порог (например, if (maskVal > 0.5) paint else original), эти зубцы будут очень заметны. Использование необработанного вероятностного вывода модели (если доступно) вместо бинаризованной маски или применение легкого размытия к бинарной маске перед передачей ее в шейдер перекраски может обеспечить более плавное альфа-смешивание по краям. Это указывает на то, что постобработка маски сегментации (например, легкое размытие, морфологические операции, как упоминается в 32 для сглаживания маски) перед ее использованием для перекраски может быть необходимым шагом для улучшения визуального качества, добавляя небольшую вычислительную стоимость, но улучшая конечный вид.
Синхронизация между генерацией маски сегментации (потенциально асинхронный инференс ML) и кадром рендеринга, в котором шейдер перекраски ее использует, имеет решающее значение для предотвращения визуальной задержки или использования "устаревшей" маски. Инференс ML-модели (II.B) может не завершаться в идеальной синхронизации с каждым кадром рендеринга, особенно на мобильных устройствах. Шейдер перекраски (III.A) выполняется каждый кадр как часть конвейера рендеринга. Если текстура маски сегментации обновляется логикой обработки вывода ML с другой скоростью или с задержкой относительно момента, когда шейдер ее считывает, окрашенная область может отставать от фактического вида с камеры или казаться "плавающей". Следовательно, архитектура системы должна тщательно управлять жизненным циклом текстуры маски сегментации, гарантируя, что шейдер перекраски всегда получает доступ к самой актуальной маске, соответствующей текущему виду с камеры, или что любая задержка обрабатывается корректно (возможно, путем предсказания движения маски, хотя это очень продвинутая техника). Это подчеркивает важность worker.PeekOutput() и асинхронной обработки данных в Sentis.19

IV. Улучшение визуальной достоверности и пользовательского опыта в ARДля создания действительно впечатляющего приложения, аналогичного Dulux Visualizer, недостаточно просто сегментировать и перекрашивать стены. Необходимо обеспечить высокую визуальную достоверность и стабильность в среде дополненной реальности.A. Использование AR FoundationAR Foundation является основным фреймворком для разработки кроссплатформенных AR-приложений в Unity.33 Он предоставляет унифицированный интерфейс к нативным AR SDK, таким как ARCore для Android и ARKit для iOS.
Основные возможности AR:

Доступ к камере: Получение видеопотока в реальном времени, который используется как для сегментации, так и для отображения фона AR-сцены.33 Компоненты ARCameraManager и ARCameraBackground управляют этой функциональностью.
Отслеживание устройства (Device Tracking): Понимание положения и ориентации устройства в физическом пространстве является основой для стабильного отображения виртуального контента.34 ARSessionOrigin отвечает за преобразование данных отслеживания в координаты сцены Unity.
Обнаружение плоскостей (Plane Detection): Хотя основную сегментацию стен выполняет ML-модель, функция обнаружения плоскостей в AR Foundation 34 может предоставлять контекстуальную информацию. Например, она может подтвердить, что большая вертикальная поверхность действительно является стеной, а также предоставить ее реальные размеры, положение и ориентацию в пространстве. Эти данные могут использоваться для дополнения или валидации результатов ML-сегментации, а также для более точного размещения виртуальных объектов или интерфейсов.


B. Стабилизация и уточнение маски сегментации (Продвинутое исследование)Маски сегментации, получаемые от ML-моделей кадр за кадром, могут страдать от временной нестабильности, проявляющейся в виде "дрожания" или "кипения" краев, особенно при быстром движении камеры или в сложных условиях освещения.

Устранение временной нестабильности (дрожания):

Оптический поток (Optical Flow): Исследование использования техник оптического потока (например, Лукаса-Канаде 38 или Фарнебека 38) для отслеживания движения пикселей между кадрами. Эта информация о потоке может быть использована для предсказания положения маски в следующем кадре или для сглаживания ее границ во времени.

OpenCV предоставляет реализации этих алгоритмов.38 Существуют обертки для Unity или прямые реализации (например, репозиторий Keijiro OpticalFlowTest 39 демонстрирует метод Лукаса-Канаде в Unity).
Работа OAMaskFlow 41 описывает использование масок движения с учетом окклюзий, что указывает на продвинутые исследования в области уточнения сегментации с помощью потока. Использование отслеживания характерных точек методом KLT 42 и их движения для деформации/стабилизации масок является связанной концепцией.43


Временная фильтрация: Более простые подходы, такие как временное усреднение или медианная фильтрация пикселей маски в небольшом временном окне, также могут помочь уменьшить высокочастотный шум.



Деформация контура маски для улучшения прилегания (Более продвинутый уровень):

Если AR Foundation обнаруживает характерные точки (feature points) 34 или якоря плоскостей (plane anchors) 34 вблизи границ сегментированной стены, их потенциально можно использовать как контрольные точки для деформации контура 2D-маски сегментации для более точного соответствия воспринимаемой геометрии.
Это сложная задача, которая может включать:

Извлечение контура из 2D-маски.
Поиск ближайших AR-характерных точек.
Применение алгоритма деформации (например, сплайны тонкой пластины, техники деформации меша 44) для корректировки контура на основе 3D-точек, спроецированных обратно в экранное пространство.
Система SpriteMask в Unity 45 позволяет создавать динамические формы масок, но ее интеграция с выводом ML и AR-точками потребует специальной разработки.


Это направление является в высшей степени исследовательским и может быть лучше отложено для будущих этапов развития, выходящих за рамки первоначального MVP.

Реализация оптического потока для стабилизации маски 38 вносит дополнительную вычислительную нагрузку, которую необходимо сбалансировать с временем инференса модели сегментации и сложностью шейдера перекраски, особенно на мобильных устройствах. Расчет оптического потока, даже с использованием эффективных методов, таких как Лукас-Канаде, требует обработки каждого пикселя или отслеживания признаков между кадрами.38 Это увеличивает нагрузку на CPU или GPU, уже занятые нейронной сетью и рендерингом. Преимущество более гладких масок должно быть сопоставлено с потенциальным падением FPS или увеличением расхода заряда батареи. Следовательно, необходимо определить бюджет производительности. Если оптический поток критичен для пользовательского опыта, это может потребовать выбора еще более легковесной модели сегментации или упрощения шейдера перекраски для высвобождения ресурсов. В качестве альтернативы, это может быть адаптивная функция, включаемая только на более производительных устройствах.
Успешное использование характерных точек AR 34 для уточнения контуров маски 44 подразумевает тесную связь и надежное соответствие между 2D-пиксельным пространством сегментации и 3D-мировым пространством AR Foundation. Несоответствия или ошибки проекции могут ухудшить качество маски. Сегментация происходит в 2D-пространстве изображения. Характерные точки AR находятся в 3D-мировом пространстве и отслеживаются с помощью SLAM. Чтобы использовать 3D-точки для корректировки 2D-маски, эти точки должны быть точно спроецированы обратно в 2D-вид текущей камеры. Любая задержка в обновлениях SLAM, ошибки калибровки камеры или неточности в матрице проекции могут привести к тому, что 2D-спроецированные точки не будут идеально совпадать с истинным краем стены на изображении. Если такие смещенные точки используются для деформации маски, они могут "оттянуть" маску от правильной границы. Это подчеркивает зависимость от высокоточной и низколатентной системы AR-трекинга (предоставляемой AR Foundation) и точной калибровки камеры. Это также предполагает, что такая техника уточнения может быть более подвержена ошибкам, чем чисто основанные на изображении методы стабилизации, такие как оптический поток, если AR-трекинг не идеален.

C. Реалистичное освещение на перекрашенных поверхностях (Продвинутое исследование)Для достижения максимального реализма перекрашенная стена должна корректно взаимодействовать с освещением в сцене, как реальным, так и виртуальным.

Использование ARMeshManager для геометрии сцены:

На устройствах с LiDAR или другими продвинутыми датчиками глубины (например, новые модели iPad/iPhone), ARKit (через ARMeshManager в AR Foundation) может генерировать 3D-сетку (меш) окружения.29
Эта сетка включает вершины и, потенциально, нормали вершин.29
Такая 3D-геометрия предоставляет фактическую информацию о поверхности стен, а не просто 2D-сегментацию.



Доступ к данным о глубине и нормалях для расчетов освещения:

Шейдер перекраски мог бы сэмплировать эти данные AR-сгенерированной сетки (глубину и нормали) для пикселей, соответствующих сегментированной стене.
Это позволило бы перекрашенной поверхности более реалистично реагировать на виртуальные источники света, размещенные в сцене, или даже на оценки реального освещения.
Доступ к мировым нормалям из ARMeshManager в экранном шейдере сложен. Один из подходов может заключаться в рендеринге AR-меша специальным шейдером, который выводит его мировые нормали в текстуру, которую затем сэмплирует основной шейдер перекраски. Примеры ARFoundation Samples 37 включают сцены, такие как NormalMeshes и OcclusionMeshes, которые демонстрируют рендеринг данных AR-меша.
Документация URP 48 упоминает доступ к _CameraNormalsTexture, если включен предварительный проход глубины/нормалей (например, для SSAO), но это будут нормали всей сцены, а не обязательно только нормали AR-меша.



Оценка освещения AR Foundation (Light Estimation):

AR Foundation предоставляет оценки условий реального освещения: общую интенсивность (ambient intensity), цвет окружающего света (ambient color), сферические гармоники, направление основного источника света и его интенсивность.35
Эти данные могут быть переданы в шейдер перекраски как uniform-переменные для влияния на конечный цвет и яркость перекрашенной стены, делая ее лучше интегрированной с освещением реального окружения.
Например, если реальная комната тускло освещена, перекрашенный цвет также должен выглядеть более тусклым.

Опора на ARMeshManager для реалистичного освещения 29 создает неравенство в возможностях между устройствами с датчиками LiDAR/глубины и без них. Приложению потребуется запасной вариант для устройств, которые не могут предоставить детализированную сетку сцены. Реконструкция сцены ARKit через ARMeshManager наиболее эффективна с LiDAR.29 Многие устройства Android и старые iPhone не имеют LiDAR. Эти устройства могут предоставлять более разреженные облака точек или менее точную геометрию плоскостей. Если функция реалистичного освещения сильно зависит от плотных нормалей сетки из ARMeshManager, она либо не будет работать, либо будет работать плохо на устройствах без LiDAR. Следовательно, дизайн приложения должен учитывать возможность постепенного снижения качества. Для устройств без LiDAR перекраска может использовать более простую модель освещения (например, только оценку окружающего света AR Foundation 35) или иметь менее детализированное взаимодействие со светом. Это влияет на усилия по разработке (несколько путей рендеринга) и на согласованность пользовательского опыта.
Точность и детализация нормалей ARMeshManager 29, особенно на неровных или текстурированных стенах, все еще могут быть недостаточными для высокодетализированных эффектов освещения, что потенциально может привести к "низкополигональному" виду освещения на перекрашенной поверхности. Сгенерированные LiDAR сетки, хотя и хороши, не обладают бесконечной детализацией. Они захватывают макрогеометрию, но могут сглаживать мелкие текстуры или едва заметные вариации поверхности реальной стены (29 упоминает сглаживание с использованием информации о плоскостях). Если шейдер перекраски использует эти нормали напрямую для освещения, свет будет взаимодействовать с этим несколько упрощенным геометрическим представлением. Это может означать, что перекрашенная поверхность, хотя и реагирует на направление света, может не отображать те же мелкие детали теней или бликов, которые были бы у оригинальной, сильно текстурированной стены. Существует потенциальное несоответствие между высокочастотной детализацией, видимой на изображении с камеры (оригинальная текстура), и низкочастотной детализацией, доступной из нормалей AR-сетки. Управление ожиданиями пользователей или исследование техник комбинирования нормалей AR-сетки с некоторой формой карт детализации (возможно, полученных из текстуры оригинального изображения) может потребоваться для достижения первоклассного реализма, что еще больше увеличит сложность.

V. Поэтапный план исследований и разработокПоэтапный подход позволяет осуществлять итеративную разработку, снижать риски и получать раннюю обратную связь.A. Фаза 1: Базовый прототип сегментации и перекраски
Цель: Создать фундаментальный конвейер: захват изображения, сегментация и базовое применение цвета.
Задачи:

Выбрать начальную модель сегментации (например, YOLOv8n-seg из-за скорости и размера 3, или MobileUNet, если будет найдена хорошая предварительно обученная модель для стен 5).
Реализовать экспорт в ONNX и интеграцию с Unity Sentis для выбранной модели.
Разработать базовую подготовку входного тензора из видеопотока камеры Unity.
Реализовать обработку выходного тензора для генерации сырой маски сегментации (для прототипирования допустима первоначальная реализация на CPU).
Создать очень простой экранный шейдер, который принимает маску и жестко заданный цвет, применяя его как плоское наложение.


Результат: Сцена в Unity, демонстрирующая сегментацию стен в реальном времени (даже если медленно) и их перекраску одним цветом.
B. Фаза 2: Интеграция AR, оптимизация шейдера и связь маски с шейдером
Цель: Интегрировать с AR Foundation, оптимизировать шейдер перекраски и правильно связать маску.
Задачи:

Настроить базовую сцену AR Foundation (камера, сессия).
Усовершенствовать шейдер перекраски: реализовать сохранение текстуры (например, манипуляции HSL/YIQ 27), передачу выбранного пользователем цвета.
Реализовать эффективную генерацию текстуры маски на GPU и ее передачу в шейдер URP через ScriptableRendererFeature.25
Оптимизировать обработку входных/выходных тензоров с помощью Sentis для повышения производительности (асинхронные операции 19).


Результат: AR-приложение, в котором пользователи могут наводить камеру на стены, выбирать цвет и видеть их перекрашенными с сохранением текстуры, работающее с интерактивной частотой кадров на целевом мобильном устройстве.
C. Фаза 3: Исследование и интеграция продвинутых улучшений
Цель: Исследовать и внедрить функции для улучшения визуальной достоверности и стабильности.
Задачи (приоритезировать в зависимости от влияния/усилий):

Стабилизация маски: Исследовать и прототипировать оптический поток 38 или другие техники временного сглаживания для маски сегментации.
Реалистичное освещение:

Интегрировать оценку освещения AR Foundation 35 в шейдер перекраски.
(Если целевыми являются высокопроизводительные устройства) Исследовать использование данных ARMeshManager 29 для более детализированного освещения на перекрашенных поверхностях.


Дообучение: Если точность сегментации недостаточна, начать сбор датасета и дообучение выбранной модели.


Результат: Улучшенный прототип с более плавными переходами маски и более реалистичной интеграцией освещения, с четкими данными о компромиссах в производительности.
D. Ключевые показатели эффективности (KPI) для оценки
Точность сегментации: Mean Intersection over Union (mIoU) для класса "стена" на определенном тестовом наборе.
Скорость инференса: Миллисекунды на кадр для модели сегментации на целевых устройствах.
Общий FPS: Частота кадров приложения на целевых устройствах.
Задержка перекраски: Время от выбора цвета до визуального обновления на стене.
Визуальное качество: Субъективная оценка на основе реализма сохранения текстуры и освещения.
Стабильность: Субъективные и объективные показатели дрожания маски.
VI. Заключительные рекомендацииA. Резюме рекомендуемых технологий и подходов
Модель сегментации: Для начального прототипирования приоритет отдается YOLOv8n-seg из-за его скорости и малого размера 3, с планом по дообучению для сегментации стен. MobileUNet 5 рассматривается как сильная альтернатива, особенно если будут найдены качественные предварительно обученные веса для стен.
Движок для инференса: Unity Sentis.16
Перекраска: Пользовательский эффект постобработки URP / ScriptableRendererFeature 25 с кастомным HLSL-шейдером, включающим манипуляции в цветовом пространстве YIQ/HSL для сохранения текстуры/яркости.27
AR Фреймворк: AR Foundation.33
B. Приоритетные области для первоначальных исследований и разработок
Быстрое прототипирование основного конвейера сегментации (Фаза 1) для раннего выявления узких мест в производительности и ограничений по точности.
Уделение особого внимания качеству сохранения текстуры в шейдере перекраски (Фаза 2), так как это является ключевым фактором для пользовательского опыта.
C. Потенциальные риски и стратегии их минимизации
Производительность/точность сегментации на мобильных устройствах: Снижение риска путем выбора легковесных моделей, агрессивной оптимизации и планирования дообучения.
Время на создание датасета/дообучение: Снижение риска путем раннего начала сбора датасета, если дообучение представляется вероятным. Исследование существующих датасетов для трансферного обучения.
Достижение ощущения "моментальной" перекраски: Снижение риска путем оптимизации всего конвейера (асинхронные операции Sentis, операции на GPU, эффективный шейдер).
Разнообразие устройств: Снижение риска путем определения четких спецификаций целевых устройств и реализации постепенного ухудшения качества для продвинутых функций, таких как освещение на основе LiDAR.


# План работ по разработке системы сегментации и перекраски стен

## Фаза 1: Базовый прототип сегментации и перекраски
- [x] Создание базовой структуры проекта
  - [x] Создание папок Scripts, Shaders, Models
  - [x] Создание базового скрипта WallSegmentation.cs
  - [x] Создание базового шейдера WallPaint.shader
  - [x] Создание скрипта WallPaintEffect.cs для управления эффектом перекраски
- [x] Анализ существующего кода WallSegmentation.cs
- [x] Обновление ML фреймворка с Barracuda на Sentis
- [x] Выбор оптимальной модели сегментации (YOLOv8n-seg или MobileUNet)
  - [x] Анализ и проверка модели model.onnx (150 классов сегментации)
  - [x] Подготовка руководства по интеграции модели в проект
- [x] Настройка экспорта модели в ONNX формат
- [x] Реализация подготовки входных данных для модели
- [x] Обработка выходного тензора для генерации маски сегментации
- [x] Создание базового шейдера для перекраски стен (найден существующий WallPaint.shader)

## Фаза 2: Интеграция AR и оптимизация
- [x] Настройка AR Foundation камеры и сессии (уже реализовано)
- [x] Интеграция модели сегментации с AR камерой
  - [x] Обновление скрипта WallSegmentation.cs согласно руководству integration_guide.md
  - [x] Настройка параметров модели и определение индекса класса "стена"
- [x] Создание системы обработки маски сегментации для шейдера перекраски
- [x] Реализация интерфейса выбора цвета
- [x] Оптимизация производительности
  - [x] Масштабирование разрешения изображения для повышения FPS
  - [ ] Добавление опций настройки качества эффекта
- [x] Устранение утечек памяти и оптимизация использования ресурсов
  - [x] Корректная очистка тензоров и CPUTensorData
  - [x] Правильное освобождение ресурсов в OnDestroy

## Фаза 3: Улучшение пользовательского опыта
- [x] Стабилизация маски сегментации
  - [x] Реализация временной фильтрации для предотвращения "дрожания" маски
  - [x] Сглаживание краев маски в шейдере
- [x] Поддержка различных стилей перекраски
  - [x] Сохранение текстуры и освещения исходной поверхности
  - [x] Поддержка различных цветов и интенсивности перекраски
- [x] Интеграция с системой освещения AR
  - [x] Получение информации об освещении через ARLightEstimation
  - [x] Адаптация цвета перекраски к освещению сцены
- [ ] Отображение состояния приложения
  - [ ] Индикация процесса загрузки модели
  - [ ] Подсказки для пользователя о необходимых действиях
- [ ] Обработка ошибок
  - [ ] Обработка ситуаций, когда сегментация не работает
  - [ ] Предложение альтернативных действий пользователю
  
## Фаза 4: Расширение функциональности
- [x] Добавление валидации настройки AR сцены
  - [x] Создание компонента ARSetupValidator
  - [x] Автоматическое исправление типичных проблем с настройкой
- [ ] Поддержка дополнительных AR-функций
  - [x] Использование освещения ARKit для реалистичной визуализации
  - [ ] Определение материалов стен с помощью ML
- [ ] Расширение пользовательского интерфейса
  - [ ] Добавление нескольких предустановленных стилей перекраски
  - [ ] Инструменты для настройки параметров перекраски
- [ ] Сохранение и загрузка результатов
  - [ ] Сохранение выбранных цветов и настроек между сессиями
  - [ ] Экспорт результатов перекраски в формате изображения

## Фаза 5: Полировка и подготовка к релизу
- [ ] Оптимизация производительности на мобильных устройствах
- [ ] Добавление UI для выбора цветов и материалов
- [ ] Создание руководства по эксплуатации (README)
- [ ] Тестирование на различных устройствах
- [ ] Подготовка демо-видео

## Приоритетные задачи на текущий момент:
1. Добавление пакета Unity Sentis в проект
2. Интеграция YOLOv8n-seg модели с Sentis
3. Создание базовой сцены в Unity с:
   - AR Camera
   - AR Session
   - WallSegmentation скриптом
   - WallPaintEffect компонентом

## Технические заметки:
- Для стабилизации результатов сегментации рекомендуется использовать буферизацию последних N кадров и их взвешенное смешивание
- Для улучшения реализма перекраски следует сохранять текстуру исходной поверхности
- Учитывать освещение сцены для корректного отображения цвета перекраски
- Разработать механизм адаптивной настройки параметров в зависимости от условий освещения

## Текущий статус:
- Создана базовая структура проекта
- Реализованы основные скрипты и шейдеры
- Решены проблемы совместимости с новым API Unity Sentis 2.1.2
- Обновлен код WallSegmentation.cs для работы с обновленным API
- Исправлены ошибки в WallPaintEffect.cs, связанные с URP
- Реализована интеграция с Unity Sentis и моделью сегментации
- Создана базовая сцена в Unity с AR компонентами
- Настроена система обработки маски сегментации и перекраски
- Успешно интегрирована система сегментации с AR-камерой
- Создан пользовательский интерфейс для выбора цвета и настройки параметров перекраски
- Разработан инструмент SceneSetupUtility для автоматического создания и настройки AR сцены
- Исправлена проблема утечки памяти в WallSegmentation.cs:
  - Перемещена static-переменная debugMaskCounter из метода Update в поле класса
  - Добавлена корректная очистка ресурсов CPUTensorData в ExecuteModelAndProcessResult, CreateSimulatedTensor, CreateTensorFromPixels и других методах
  - Исправлена ошибка с несуществующим методом PeekConstants()
  - Улучшен метод OnDestroy для более полного освобождения ресурсов
  - Добавлена очистка текстур и вызов GC.Collect()
- Исправлена проблема с отсутствием материалов для AR-плоскостей:
  - Добавлена автоматическая настройка материалов для AR Plane в редакторе (SceneSetupUtility.cs)
  - Реализовано автоматическое назначение материалов для динамически создаваемых AR-плоскостей

## Следующие шаги:
1. Добавить опции настройки качества эффекта перекраски
2. Начать работу над стабилизацией сегментации при движении камеры
3. Добавить поддержку различных материалов для более реалистичной визуализации
4. Протестировать систему на различных устройствах

# Чек-лист устранения черного экрана и ошибок Sentis

1. **Проверить базовые настройки Unity:**
   - Сцена добавлена в Build Settings и активна.
   - В сцене есть активная Main Camera с правильным Culling Mask и Clear Flags.
   - Нет критических ошибок в Awake/Start у ключевых объектов (проверить консоль).
   - Проверить совместимость Graphics API (например, Metal, Vulkan, DirectX11/12).

2. **Проверить настройки URP (если используется):**
   - URP Asset назначен в Project Settings > Graphics.
   - Forward Renderer/Renderer Data присутствует и не поврежден.
   - Отключить поочередно кастомные Renderer Features для поиска проблемного.
   - Все используемые шейдеры совместимы с URP.
   - Все входные текстуры для кастомных RenderPass корректно назначены.

3. **Проверить AR Foundation (если используется):**
   - Приложение запрашивает разрешение на камеру (AndroidManifest/Info.plist).
   - ARSession, ARCameraManager и XR Plug-in Management настроены.
   - Устройство поддерживает ARCore/ARKit.
   - Нет ошибок инициализации AR в консоли.

4. **Проверить интеграцию Sentis:**
   - Установлен актуальный пакет com.unity.sentis, версия совместима с Unity.
   - Весь код использует новые API Sentis 2.x (Tensor<float>, Worker, worker.Schedule и т.д.).
   - Нет упоминаний TensorFloat, WorkerFactory, worker.Execute и других устаревших методов.
   - Модель загружается корректно (ModelLoader.Load не возвращает null, путь к модели корректен для платформы).
   - Для StreamingAssets на Android/WebGL используется UnityWebRequest для загрузки модели.
   - Проверить, что runtimeModel и Worker не null после загрузки.

5. **Проверить подготовку входного тензора:**
   - Размеры и формат inputTensor соответствуют ожиданиям модели (shape, channels, batch).
   - Используется TextureConverter.ToTensor с нужными параметрами.
   - Нормализация входных данных соответствует обучению модели.

6. **Проверить вывод модели:**
   - После worker.Schedule() получить outputTensor и сделать ReadbackAndClone().
   - Проверить shape и значения outputTensor (не все нули, диапазон значений адекватный).
   - Для многоклассовой сегментации — применить argmax по классу, для бинарной — threshold.

7. **Проверить визуализацию маски:**
   - Использовать TextureConverter.ToTexture с правильным TextureTransform (SetBroadcastChannels(true) для grayscale).
   - RenderTexture с маской назначен на RawImage или материал, который виден камере.
   - Если используется кастомный шейдер — проверить его на ошибки компиляции, корректность UV и threshold.
   - Материал поддерживает прозрачность, если требуется альфа-смешивание.
   - RenderPass в URP настроен на правильный RenderPassEvent.

8. **Пошаговая отладка:**
   - Начать с минимальной сцены и статического изображения, добиться работы Sentis.
   - Проверять логи на каждом этапе, устранять все ошибки и предупреждения.
   - Постепенно добавлять сложность: камера, AR, кастомные шейдеры, оптимизации.
   - Использовать Frame Debugger и Profiler для анализа рендеринга и производительности.

9. **Рекомендации по производительности и памяти:**
   - Использовать минимально возможную модель, рассмотреть квантование (UInt8).
   - Выбирать оптимальный BackendType (GPUCompute, CPU, GPUPixel) для целевого устройства.
   - Использовать worker.ScheduleIterable для больших моделей.
   - Минимизировать копирование данных между CPU и GPU.
   - Всегда вызывать Dispose() для Tensor и Worker, освобождать RenderTexture.

10. **Если проблема не решается:**
   - Сформулировать минимальный воспроизводимый пример.
   - Описать версию Unity, Sentis, платформу, логи ошибок.
   - Поделиться информацией на форумах Unity (раздел Sentis) или Hugging Face.
